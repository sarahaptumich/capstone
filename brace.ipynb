{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOqyowK8SErg8L6NdF1umbx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarahaptumich/capstone/blob/shaptonstall/brace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Q_c2JZ5xGiW",
        "outputId": "61d677b3-6e65-440e-d099-7e765a59ec45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Github/capstone/brace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjUD3TpP3nCf",
        "outputId": "1c83447d-9acd-4291-a87b-60e32a608abb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Github/capstone/brace\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZktF17T3pIo",
        "outputId": "d6020e8f-b10e-402d-b385-c0f76cc1aa86"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mannotations\u001b[0m/    brace_test.pth   brace_train.pth  dataset_pytorch.py  \u001b[01;34m__pycache__\u001b[0m/  \u001b[01;34mutils\u001b[0m/\n",
            "brace_test.pkl  brace_train.pkl  \u001b[01;34mdataset\u001b[0m/         \u001b[01;34mfigures\u001b[0m/            README.md     videos_info.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !nvidia-smi"
      ],
      "metadata": {
        "id": "qsLkZ6u4TAyK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Github/capstone/brace')"
      ],
      "metadata": {
        "id": "SgLOxprD3ypr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn, optim\n",
        "import dataset_pytorch\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from torch.utils.data import TensorDataset\n",
        "from tqdm import tqdm\n",
        "from pathlib import PosixPath\n",
        "import pickle"
      ],
      "metadata": {
        "id": "-1pDE0Lr33BQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset, only need to use once since models are saved\n",
        "\n",
        "def prepare_data():\n",
        "    # Load data and create datasets\n",
        "    sequences_path_ = Path('/content/drive/MyDrive/Github/capstone/brace/dataset')\n",
        "    df_ = pd.read_csv(Path('/content/drive/MyDrive/Github/capstone/brace/annotations/sequences.csv'))\n",
        "\n",
        "    train_df = pd.read_csv('/content/drive/MyDrive/Github/capstone/brace/annotations/sequences_train.csv')\n",
        "    train_df = df_[df_.uid.isin(train_df.uid)]\n",
        "\n",
        "    brace_train = dataset_pytorch.BraceDataset(sequences_path_, train_df)\n",
        "    print(f'Loaded BRACE training set! We got {len(brace_train)} training sequences')\n",
        "    skeletons_train, metadata_train = brace_train.__getitem__(0)\n",
        "    print(metadata_train)\n",
        "\n",
        "    test_df = pd.read_csv('/content/drive/MyDrive/Github/capstone/brace/annotations/sequences_test.csv')\n",
        "    test_df = df_[df_.uid.isin(test_df.uid)]\n",
        "\n",
        "    brace_test = dataset_pytorch.BraceDataset(sequences_path_, test_df)\n",
        "    print(f'Loaded BRACE test set! We got {len(brace_test)} testing sequences')\n",
        "    skeletons_test, metadata_test = brace_test.__getitem__(0)\n",
        "    print(metadata_test)\n",
        "\n",
        "    return brace_train, brace_test, skeletons_train, metadata_train, skeletons_test, metadata_test\n",
        "\n",
        "brace_train, brace_test, skeletons_train, metadata_train, skeletons_test, metadata_test = prepare_data()\n",
        "\n",
        "# Continue with the rest of the code (define model, train, test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3X7xTmuY2XCL",
        "outputId": "56e7b351-4128-422e-ed62-ab122003f6b9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading BRACE: 100%|██████████| 319/319 [00:20<00:00, 15.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Going to sample 900 frames from each sequence. Average sequence length is 730.7711598746082\n",
            "Loaded BRACE training set! We got 319 training sequences\n",
            "{'video_id': '3rIk56dcBTM', 'seq_idx': 1, 'start_frame': 1751, 'end_frame': 2915, 'dancer': 'roxrite', 'dancer_id': 47, 'year': 2011, 'uid': '3rIk56dcBTM.1'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading BRACE: 100%|██████████| 146/146 [00:09<00:00, 14.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Going to sample 900 frames from each sequence. Average sequence length is 690.9794520547945\n",
            "Loaded BRACE test set! We got 146 testing sequences\n",
            "{'video_id': '3rIk56dcBTM', 'seq_idx': 0, 'start_frame': 851, 'end_frame': 1645, 'dancer': 'el niño', 'dancer_id': 10, 'year': 2011, 'uid': '3rIk56dcBTM.0'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save the entire dataset object\n",
        "# with open('brace_train.pkl', 'wb') as f:\n",
        "#     pickle.dump(brace_train, f)\n",
        "\n",
        "# with open('brace_test.pkl', 'wb') as f:\n",
        "#     pickle.dump(brace_test, f)"
      ],
      "metadata": {
        "id": "VQSmJz5Tty6l"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use this to load the dataset"
      ],
      "metadata": {
        "id": "rt-EqlkRYyQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the entire dataset object\n",
        "with open('brace_train.pkl', 'rb') as f:\n",
        "    brace_train = pickle.load(f)\n",
        "\n",
        "with open('brace_test.pkl', 'rb') as f:\n",
        "    brace_test = pickle.load(f)"
      ],
      "metadata": {
        "id": "VVLA1zmKu66Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train= brace_train.sequences\n",
        "y_train=brace_train.clip_labels\n",
        "x_test = brace_test.sequences\n",
        "y_test=brace_test.clip_labels\n",
        "len(x_train), len(y_train), len(x_test), len(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2TMHZeACCL-",
        "outputId": "ef314b87-c4b8-4e2b-b048-ca87b7803762"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(319, 319, 146, 146)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert NumPy arrays to PyTorch tensors\n",
        "x_train_tensors = [torch.tensor(seq) for seq in x_train]\n",
        "x_test_tensors = [torch.tensor(seq) for seq in x_test]\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "padded_x_train = rnn_utils.pad_sequence(x_train_tensors, batch_first=True)\n",
        "padded_x_test = rnn_utils.pad_sequence(x_test_tensors, batch_first=True)\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "y_train_array = np.array(y_train)\n",
        "y_test_array = np.array(y_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "y_train_tensor = torch.tensor(y_train_array)\n",
        "y_test_tensor = torch.tensor(y_test_array)\n",
        "\n",
        "# Create datasets and data loaders using padded sequences\n",
        "train_dataset = TensorDataset(padded_x_train, y_train_tensor)\n",
        "test_dataset = TensorDataset(padded_x_test, y_test_tensor)\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=None, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=None, shuffle=False)"
      ],
      "metadata": {
        "id": "8TRoKh0P1aj0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JwbV2Rbv1Iqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "k329g0DixKLI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "gamZGf7QxNrw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a DataLoader named train_loader\n",
        "\n",
        "# Instantiate your LSTMModel\n",
        "input_size = 2  # adjust based on your input data\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "num_classes = 3  # adjust based on your dataset\n",
        "lstm_model = LSTMModel(input_size, hidden_size, num_layers, num_classes)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
        "\n",
        "# Number of epochs\n",
        "num_epochs = 10\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    lstm_model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Iterate over the data\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        # Forward pass\n",
        "        batch_x = batch_x.float()\n",
        "        batch_y = torch.full((batch_x.size(0),), batch_y.item(), dtype=torch.long)\n",
        "\n",
        "        outputs = lstm_model(batch_x)\n",
        "        # print(batch_y.shape, outputs.shape)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * batch_x.size(0)\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "    # Print loss (optional)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCx1VGrPwUCS",
        "outputId": "e3b02e17-9fe9-40df-923f-705f20f62102"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1588.8892\n",
            "Epoch [2/10], Loss: 1520.1132\n",
            "Epoch [3/10], Loss: 1521.6549\n",
            "Epoch [4/10], Loss: 1520.8709\n",
            "Epoch [5/10], Loss: 1519.7987\n",
            "Epoch [6/10], Loss: 1518.9312\n",
            "Epoch [7/10], Loss: 1517.8251\n",
            "Epoch [8/10], Loss: 1516.7940\n",
            "Epoch [9/10], Loss: 1515.6855\n",
            "Epoch [10/10], Loss: 1514.5620\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the file path where you want to save the model\n",
        "model_path = 'lstm_model.pth'\n",
        "\n",
        "# # Save the model state dictionary\n",
        "# torch.save(lstm_model.state_dict(), model_path)\n",
        "\n",
        "# print(\"Model saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac2T2fEQZLaW",
        "outputId": "149deb0c-cb86-4918-c504-9eece5512315"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "loaded_model = LSTMModel(input_size, hidden_size, num_layers, num_classes)\n",
        "\n",
        "# Load the model state dictionary\n",
        "loaded_model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "print(\"Model loaded successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28hYuWU8pgy4",
        "outputId": "fcaf8c66-19f4-4551-b043-8bd912f2b3d0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s64_9n-RpocJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a DataLoader named test_loader\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "lstm_model.eval()\n",
        "\n",
        "# Initialize variables to track accuracy and loss\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "total_loss = 0.0\n",
        "\n",
        "# Disable gradient calculation during testing\n",
        "with torch.no_grad():\n",
        "    for batch_x, batch_y in test_loader:\n",
        "        # Forward pass\n",
        "        batch_x = batch_x.float()\n",
        "        batch_y = torch.full((batch_x.size(0),), batch_y.item(), dtype=torch.long)\n",
        "\n",
        "        outputs = lstm_model(batch_x)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        # Calculate total loss\n",
        "        total_loss += loss.item() * batch_x.size(0)\n",
        "\n",
        "        # Get predictions\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        # Update total number of samples and correct predictions\n",
        "        total_correct += (predicted == batch_y).sum().item()\n",
        "        total_samples += batch_x.size(0)\n",
        "\n",
        "# Calculate average loss and accuracy\n",
        "average_loss = total_loss / total_samples\n",
        "accuracy = total_correct / total_samples\n",
        "\n",
        "# Print test results\n",
        "print(f'Test Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SaJeqfxRXUo",
        "outputId": "2b740b81-cd58-487d-b05f-2a5f8090774c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.7476, Accuracy: 0.5959\n"
          ]
        }
      ]
    }
  ]
}